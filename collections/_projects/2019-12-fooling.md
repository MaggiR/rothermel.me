---
title: "Fooling an image classifier with adversarial inputs"
date: 2019-12-15
years: 2019
image-sm:
  path: assets/images/projects/saliency_map.jpg
  alt: "Saliency map"
tags: ml xai
links:
- target: https://colab.research.google.com/drive/1McqdMbBfIh4QMiVdPO06v44O5OsYUAos
  text: Interactive Source
  icon: fas fa-code
---

This little experiment showcases how easy neural
network image classifiers can be fooled with a slight
perturbation in the image. It also explains and shows
**Saliency Maps** (heatmaps indicating the classifier's
attention in the image when classifying it).
