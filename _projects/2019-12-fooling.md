---
title: "Fooling an image classifier with adversarial inputs"
date: 2020-08-31
image-sm:
  path: assets/images/projects/saliency_map.jpg
  alt: "Saliency map"
---

This little experiment showcases how easy neural
network image classifiers can be fooled with a slight
perturbation in the image. It also explains and shows
**Saliency Maps** (heatmaps indicating the classifier's
attention in the image when classifying it). You can try it
out with directly online by yourself directly on
[Google Colab](https://colab.research.google.com/drive/1McqdMbBfIh4QMiVdPO06v44O5OsYUAos).
